<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <title>Produce Freshness Estimation: Ready to Pick, Ready to Eat, or Spoiled</title>
    <meta name="description" content="A hands-on tutorial on how computer vision estimates produce freshness: sensors, algorithms, challenges, and a mini-demo + quiz." />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap" rel="stylesheet" />
    <link rel="stylesheet" href="assets/styles.css" />
    <script defer src="assets/script.js"></script>
    <!-- Optional ML in browser (for the TF.js demo fallback) -->
    <script defer src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.20.0/dist/tf.min.js"></script>
</head>

<body>
<header class="site-header">
    <div class="brand">
        <span class="emoji">üçå</span>
        <h1>Produce Freshness Estimation</h1>
    </div>
    <nav class="top-nav" aria-label="Primary">
        <a href="#intro">Introduction</a>
        <a href="#sensors">Sensors</a>
        <a href="#algorithms">Algorithms</a>
        <a href="#success">Successful vs Less Successful</a>
        <a href="#challenges">Challenges</a>
        <a href="#future">Future & Conclusion</a>
        <a href="#demo">Mini Demo</a>
        <a href="#quiz">Quiz</a>
    </nav>
</header>

<main>
    <!-- Hero / Intro Page -->
    <section id="hero" class="hero">
        <div class="hero-copy">
            <h2>Ready to Pick, Ready to Eat, or Spoiled?</h2>
            <p>Explore how <strong>computer vision</strong> helps growers, grocers, and consumers estimate freshness and ripeness using cameras and algorithms. This interactive tutorial covers sensors, models, pitfalls, and what‚Äôs next.</p>
        </div>
    </section>

    <!-- Introduction -->
    <section id="intro" class="content-section">
        <h3>Introduction & Problem Definition</h3>
        <p>    Determining ‚Äúfreshness‚Äù blends the evaluation and classification of visible cues (color, texture, bruising),
            internal chemistry (sugars, acids, volatiles), and physical exposure (time and temperature)
            [<a href="#ref-1">1</a>]. Evaluation of these attributes typically involves using sensors‚Äîvisual, chemical, or
            physical‚Äîto measure these characteristics; some techniques involve sampling flesh or juice, measuring pH with
            electrodes, or using indicators/labels that reflect temperature history
            [<a href="#ref-1">1</a>]. However, much current research focuses on <em>non-destructive</em> optical methods that
            analyze reflected/transmitted light (e.g., VIS‚ÄìNIR and hyperspectral imaging) to infer internal quality without
            damaging or even touching the fruit [<a href="#ref-2">2</a>]. Many of the biggest challenges involve outdoor and
            operational factors (lighting changes, occlusions, season/cultivar shifts) that degrade image quality and model
            generalization; modern computer vision with deep learning addresses these issues and maps visual cues (color,
            texture, shape, size) to supervised targets such as unripe/ripe/overripe using labeled images and videos
            [<a href="#ref-3">3</a>]. These vision approaches are attractive in production because they are fast, scalable,
            non-destructive, and increasingly cost-effective at the edge [<a href="#ref-2">2</a>, <a href="#ref-3">3</a>].</p>
        <!-- Bananas: Ripeness Examples with Demo Legends -->
        <style>
          .banana-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
            gap: 16px;
            align-items: start;
            margin: 1rem 0 2rem;
          }
          .banana-grid figure {
            margin: 0;
            border: 1px solid #e5e7eb;
            border-radius: 12px;
            overflow: hidden;
            background: #fff;
            box-shadow: 0 2px 6px rgba(0,0,0,0.06);
          }
          .banana-grid img {
            width: 100%;
            height: 180px;
            object-fit: cover;
            display: block;
          }
          .banana-grid figcaption {
            padding: 10px 12px 12px;
            font-size: 14px;
            line-height: 1.35;
          }
          .banana-title {
            font-weight: 600;
            margin-bottom: 4px;
          }
          .banana-legend {
            font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
            font-size: 12px;
            background: #f3f4f6;
            border-radius: 6px;
            padding: 6px 8px;
            margin: 6px 0 8px;
            display: inline-block;
          }
          .banana-attr {
            font-size: 12px;
            color: #6b7280;
          }
          .banana-attr a { color: #6b7280; text-decoration: underline; }
        </style>

        <div class="banana-grid">
            <!-- Unripe (green) -->
            <figure>
                <img
                        src="https://upload.wikimedia.org/wikipedia/commons/6/66/Banana_tree_with_green_bananas.jpg"
                        alt="Green bananas on a banana plant" loading="lazy">
                <figcaption>
                    <div class="banana-title">Unripe (Green on Plant)</div>
                    <div class="banana-legend">Predicted: <strong>Unripe</strong> ‚Äî confidence 0.96</div>
                    <div class="banana-attr">
                        Source: <a href="https://commons.wikimedia.org/wiki/File:Banana_tree_with_green_bananas.jpg">Wikimedia Commons</a> ¬∑ Public Domain (author: Rosendahl)
                    </div>
                </figcaption>
            </figure>

            <!-- Ready to Eat (single Cavendish) -->
            <figure>
                <img
                        src="https://upload.wikimedia.org/wikipedia/commons/8/8a/Banana-Single.jpg"
                        alt="Single ripe Cavendish banana on white background" loading="lazy">
                <figcaption>
                    <div class="banana-title">Ready to Eat (Fully Yellow)</div>
                    <div class="banana-legend">Predicted: <strong>Ripe</strong> ‚Äî confidence 0.92</div>
                    <div class="banana-attr">
                        Source: <a href="https://commons.wikimedia.org/wiki/File:Banana-Single.jpg">Wikimedia Commons</a> ¬∑ ¬© Evan-Amos ¬∑ License: CC BY-SA 3.0
                    </div>
                </figcaption>
            </figure>

            <!-- Ready to Eat (spotty / sweeter) -->
            <figure>
                <img
                        src="https://upload.wikimedia.org/wikipedia/commons/8/8e/Banana_Fruit.JPG"
                        alt="Ripe bananas with brown speckles on white background" loading="lazy">
                <figcaption>
                    <div class="banana-title">Ready to Eat (Spotty / Sweeter)</div>
                    <div class="banana-legend">Predicted: <strong>Ripe</strong> ‚Äî confidence 0.88</div>
                    <div class="banana-attr">
                        Source: <a href="https://commons.wikimedia.org/wiki/File:Banana_Fruit.JPG">Wikimedia Commons</a> ¬∑ Public Domain (author: ZooFari)
                    </div>
                </figcaption>
            </figure>

            <!-- Overripe / Spoiled -->
            <figure>
                <img
                        src="https://upload.wikimedia.org/wikipedia/commons/5/51/Extremely_overripe_banana.jpg"
                        alt="Extremely overripe banana with dark peel" loading="lazy">
                <figcaption>
                    <div class="banana-title">Overripe / Spoiled</div>
                    <div class="banana-legend">Predicted: <strong>Overripe/Spoiled</strong> ‚Äî confidence 0.97</div>
                    <div class="banana-attr">
                        Source: <a href="https://commons.wikimedia.org/wiki/File:Extremely_overripe_banana.jpg">Wikimedia Commons</a> ¬∑ ¬© Park taeho ¬∑ License: CC BY-SA 4.0
                    </div>
                </figcaption>
            </figure>
        </div>
    </section>

    <!-- Sensors -->
    <section id="sensors" class="content-section">
        <h3>Overview of Commonly Used Sensor Techniques</h3>
        <p>There are a number of existing techniques used in industry for detecting spoilage.

        <h4>Images and Video</h4>
        <p>    Most field and retail setups use standard RGB cameras on small, battery-powered devices (e.g., phones, action cameras).
            Hardware is affordable, easy to deploy, and reliable. However, this technique typically requires more strongly ideal
            conditions than others; for example, some setups rely on sunlight to reflect off of the produce
            [<a href="#ref-4">4</a>].</p>
        <ul class="bullets">
            <li><strong>Components:</strong> camera, lighting, neutral background, compute (device or cloud).</li>
            <li><strong>Use conditions:</strong> diffuse light, fixed distance, minimal motion blur; white-balance and exposure locked (to have more consistent coloring.)</li>
            <li><strong>Example output format(s):</strong> JPEG/PNG visual and MP4 model output with label confidence and bounding boxes.</li>
            <li><strong>Limitations:</strong> Only examines external conditions. Sensitive to lighting, specular glare, and other lighting conditions.</li>
        </ul>
        <p></p>
        <p></p>
        <style>
          .two-up { display:grid; grid-template-columns: repeat(auto-fit, minmax(260px,1fr)); gap:16px; }
          figure { margin:0; border:1px solid #e5e7eb; border-radius:12px; overflow:hidden; background:#fff; }
          img { width:100%; height:220px; object-fit:cover; display:block; }
          figcaption { padding:10px 12px 12px; font:14px/1.4 system-ui, -apple-system, Segoe UI, Roboto, "Helvetica Neue", Arial, sans-serif; }
          .title { font-weight:600; margin-bottom:4px; }
          .legend { font:12px/1.3 ui-monospace, SFMono-Regular, Menlo, Consolas, monospace; background:#f3f4f6; border-radius:6px; padding:6px 8px; display:inline-block; margin:6px 0 8px; }
          .attr { font-size:12px; color:#6b7280; }
          .attr a { color:#6b7280; text-decoration:underline; }
        </style>

        <div class="two-up">
            <!-- Image 1: Still image (RGB) showing multiple ripeness states -->
            <figure>
                <img
                        src="https://commons.wikimedia.org/wiki/Special:FilePath/Approximately%2030%20Gros%20Michel%20Bananas.jpg"
                        alt="Bananas on a countertop in multiple ripeness stages from green to yellow to spotted">
                <figcaption>
                    <div class="title">RGB Still Image (Mobile): Multiple Ripeness States</div>
                    <div class="legend">Demo (per-fruit): Unripe 0.11 ¬∑ Turning 0.17 ¬∑ Ripe 0.62 ¬∑ Overripe 0.10</div>
                    <div class="attr">
                        Source: ‚ÄúApproximately 30 Gros Michel Bananas‚Äù via Wikimedia Commons (CC BY 4.0) ‚Äî photo by Zwifree.
                    </div>
                </figcaption>
            </figure>

            <!-- Image 2: Representative video frame -->
            <figure>
                <img
                        src="https://commons.wikimedia.org/wiki/Special:FilePath/Extremely%20overripe%20banana.jpg"
                        alt="Close-up of an extremely overripe banana with dark peel">
                <figcaption>
                    <div class="title">Video (Smartphone): Representative Frame</div>
                    <div class="legend">Demo (frame-level): Predicted = <strong>Overripe</strong> ‚Äî confidence 0.97</div>
                    <div class="attr">
                        Source: ‚ÄúExtremely overripe banana‚Äù via Wikimedia Commons (CC BY-SA 4.0) ‚Äî photo by Park taeho.
                    </div>
                </figcaption>
            </figure>
        </div>

        <h4>Multispectral and Hyperspectral</h4>
        <p><strong>Hyperspectral imaging</strong> uses both spectroscopy and imaging to build a per-pixel ‚Äúchemical map‚Äù of fruit; modern systems capture dozens‚Äìhundreds of narrow bands and correlate spectra to internal chemistry, and snapshot cameras are enabling more real-time quality control
        [<a href="#ref-5">5</a>].</p>
        <ul class="bullets">
            <li><strong>Components:</strong> HSI camera (push-broom or snapshot), lens, uniform lighting, calibration targets (white/dark), conveyance for line-scan systems.</li>
            <li><strong>Use conditions:</strong> stable illumination, controlled motion for push-broom, periodic white/dark reference captures.</li>
            <li><strong>Output:</strong> Classification outputs.</li>
            <li><strong>Limitations:</strong> Cost and complexity are significantly higher than basic RGB setups, calibration requirements are more severe. Many of the same issues with less than ideal conditions as RGB has.</li>
        </ul>
        <p></p><p></p>
        <style>
          .hs-two { display:grid; grid-template-columns: repeat(auto-fit, minmax(300px,1fr)); gap:16px; }
          .hs-two figure { margin:0; border:1px solid #e5e7eb; border-radius:12px; overflow:hidden; background:#fff; }
          .hs-two img { width:30%; height:auto; display:block; }
          .hs-two figcaption { padding:10px 12px 12px; font:14px/1.4 system-ui, -apple-system, Segoe UI, Roboto, "Helvetica Neue", Arial, sans-serif; }
          .hs-title { font-weight:600; margin-bottom:4px; }
          .hs-attr { font-size:12px; color:#6b7280; margin-top:6px; }
          .hs-attr a { color:#6b7280; text-decoration:underline; }
        </style>

        <div class="hs-two">
            <!-- Image A: Fruit ripeness ‚Äúchemical map‚Äù from hyperspectral data -->
            <figure>
                <img
                        src="https://www.mdpi.com/agriculture/agriculture-12-02145/article_deploy/html/images/agriculture-12-02145-g007.png"
                        alt="Achacha fruit shown in RGB alongside false-color hyperspectral ripeness maps estimated by machine learning">
                <figcaption>
                    <div class="hs-title">Hyperspectral ripeness maps (pixel-wise)</div>
                    <div>
                        Example of **RGB vs. false-color ripeness maps** derived from hyperspectral imaging; colors reflect estimated ripeness level per pixel using regression models (PLSR/SVR).
                    </div>
                    <div class="hs-attr">
                        Source: Nguyen &amp; Liou, <em>Agriculture</em> (MDPI), 2022. Licensed CC BY 4.0.
                        <a href="https://www.mdpi.com/2077-0472/12/12/2145">Article page</a>.
                    </div>
                </figcaption>
            </figure>
        </div>

        <h4>Near-infrared Spectroscopy</h4>
        <p>Near-infrared spectroscopy measures the absorption of infrared light by various chemical bonds which are released by the ripening process, and uses this information to determine approximately how far the produce is into the ripening process.
        <ul class="bullets" role="list">
            <li>
                <strong>Components:</strong>
                A handheld or inline near-infrared unit with a stable IR light source, a spectrometer/detector and probe optics, plus a controller that applies calibration (white/dark references) and an ML model to the measured spectrum.
            </li>
            <li>
                <strong>Use conditions:</strong>
                Works best with clean, matte surfaces at a fixed distance with minimal ambient light and temperature-stable hardware
            </li>
            <li>
                <strong>Output:</strong>
                A non-destructive estimate of ripeness such as a maturity score or predicted values often accompanied by a confidence or quality flag.
            </li>
            <li>
                <strong>Limitations:</strong>
                Shallow penetration depth and sensitivity to glare, water, wax, dirt, and season shifts require that careful calibration and ground-truthing are done. Also, hardware cost is higher than simple RGB cameras.
            </li>
        </ul>
        <p></p>
        <!-- Near-infrared Spectroscopy (NIR) ‚Äî Example Image -->
        <figure style="margin:0; border:1px solid #e5e7eb; border-radius:12px; overflow:hidden; background:#fff; max-width:760px;">
            <img
                    src="https://pub.mdpi-res.com/applsci/applsci-11-03209/article_deploy/html/images/applsci-11-03209-g001.png"
                    alt="Handheld near-infrared spectrometer measuring an intact tomato while a laptop displays its spectrum"
                    style="width:100%; height:auto; display:block;">
            <figcaption style="padding:10px 12px 12px; font:14px/1.45 system-ui, -apple-system, Segoe UI, Roboto, 'Helvetica Neue', Arial, sans-serif;">
                <strong>Near-infrared Spectroscopy (NIR), non-destructive:</strong>
                A handheld spectrometer illuminates the peel with NIR light; wavelength-specific absorption from O‚ÄìH/C‚ÄìH/N‚ÄìH bonds is modeled to estimate ripeness-related chemistry (e.g., ¬∞Brix/SSC) without cutting the fruit.
                <div style="font-size:12px; color:#6b7280; margin-top:6px;">
                    Image: Borba et&nbsp;al., <em>Applied Sciences</em> (MDPI), 2021 ‚Äî CC BY&nbsp;4.0.
                    <a href="https://www.mdpi.com/2076-3417/11/7/3209">Article</a> ¬∑
                    <a href="https://pub.mdpi-res.com/applsci/applsci-11-03209/article_deploy/html/images/applsci-11-03209-g001.png">Figure&nbsp;1</a>
                </div>
            </figcaption>
        </figure>

        <h4>Fluorescence Spectroscopy</h4>
        <p>Fluorescence spectroscopy is a technique which uses fluorescent light emitted by fluorophores, which can be used to measure microbial growth and oxidation within the produce.[<a href="#ref-6">6</a>]</p>
        <ul class="bullets" role="list">
            <li>
                <strong>Components:</strong>
                A UV/blue excitation source such as an LED or laser, collection optics and a spectrometer or fluorescence camera, plus control software for calibration and analysis.
            </li>
            <li>
                <strong>Use conditions:</strong>
                Best used in a dark, light-controlled setup with fixed geometry and integration time, clean/matte surfaces, and stable temperature.
            </li>
            <li>
                <strong>Output:</strong>
                An emission spectrum or false-color fluorescence image/ratio map that is converted to a spoilage or oxidation index and an optional confidence/quality flag.
            </li>
            <li>
                <strong>Limitations:</strong>
                Signals are surface-biased and can be weak or quenched, bands often overlap and require careful calibration, wet/waxy skins and ambient light degrade accuracy, photobleaching can occur, and hardware is more complex/costly than RGB.
            </li>
        </ul>


<!--        <div class="note">-->
<!--            <strong>Further reading/videos:</strong>-->
<!--            <ul>-->
<!--                <li>HSI concept: <a href="https://en.wikipedia.org/wiki/Hyperspectral_imaging" target="_blank" rel="noopener">Wikipedia overview</a>.</li>-->
<!--                <li>Fruit maturity via HSI (review & demos): <a href="https://www.mdpi.com/2076-3417/13/17/9740" target="_blank" rel="noopener">MDPI Appl. Sci. 2023</a>; <a href="https://www.sciencedirect.com/science/article/pii/S2589721720300131" target="_blank" rel="noopener">Real-time HSI for strawberries</a>.</li>-->
<!--                <li>Cubert palm-oil ripeness demo (video article): <a href="https://cubert-hyperspectral.com/en/palm-oil-fruit-ripeness/" target="_blank" rel="noopener">link</a>.</li>-->
<!--            </ul>-->
<!--        </div>-->
    </section>

    <!-- Algorithms -->
    <section id="algorithms" class="content-section">
        <h3>Algorithms Involved in Detecting Freshness</h3>
        <p><strong>Color/texture Classification and Basic ML</strong>: This algorithm involves converting images to color spaces that are perceived by human perception, computing simple stats from these spaces like means and histograms, and adding texture descriptors. A lightweight classifier then maps features to ripeness tiers. This is especially effective under ideal conditions, such as fixed and consistent lighting, neutral backgrounds and with produce that has consistent and predictable color progression.[<a href="#ref-7">7</a>]</p>
        <p><strong>Pros:</strong>
        <ul class="bullets">
            <li>Models are simple and can easily run on small devices.</li>
            <li>Easy to interpret and calibrate, with minimal training data required.</li>
        </ul>
        <p><strong>Cons:</strong>
        <ul class="bullets">
            <li>Requires ideal conditions, such as good illumination and consistent color balance.</li>
            <li>Not good at detecting subtle defects or internals of the fruit.</li>
        </ul>
        <p></p>
        <details>
            <summary style="cursor:pointer;font-weight:600"><strong>Click to see algorithm</strong></summary>
            <pre><code>
                // Capture & setup (controlled environment)
                lock_camera_settings(exposure, white_balance, ISO)
                img = capture_frame()                      // RGB image under diffuse, high-CRI LEDs

                // Preprocess
                img = gray_world_white_balance(img)        // simple color constancy
                mask = segment_foreground(img)             // e.g., Otsu on Lab a*, clean with morphology
                mask = remove_glare(mask, img)             // suppress V&gt;0.95 &amp; S&lt;0.15 pixels in HSV

                // Feature extraction (masked)
                hsv  = to_HSV(img); lab = to_Lab(img); gray = to_gray(img)
                H_hist = histogram(hsv.H[mask], bins=16)   // hue histogram (wrap-around)
                S_hist = histogram(hsv.S[mask], bins=8)
                V_hist = histogram(hsv.V[mask], bins=8)
                Lab_stats = [mean(lab.L[mask]), std(lab.L[mask]),
                             mean(lab.a[mask]), std(lab.a[mask]),
                             mean(lab.b[mask]), std(lab.b[mask])]
                LBP_hist = lbp_uniform_hist(gray[mask], P=8, R=1)
                GLCM = glcm(gray[mask], dist=1, angle=0)   // co-occurrence matrix
                GLCM_feats = [contrast(GLCM), homogeneity(GLCM), energy(GLCM), correlation(GLCM)]
                dark_ratio = pct(gray[mask] &lt; percentile(gray[mask], 15))
                shape_feats = [area(mask), compactness(mask)]

                // Assemble feature vector
                x = concat(H_hist, S_hist, V_hist, Lab_stats, LBP_hist, GLCM_feats, dark_ratio, shape_feats)

                // Train (offline)
                X_train, y_train = load_dataset_features(...)
                clf = StandardScaler() + LogisticRegression()   // or LinearSVM / RandomForest
                clf.fit(X_train, y_train)                       // keep a validation set for tuning/calibration

                // Inference (online)
                y_hat = clf.predict(x)
                p_hat = clf.predict_proba(x)                    // use calibrated probabilities if available
                return {class: y_hat, confidence: max(p_hat)}

                // Maintenance
                // - Periodic re-check of calibration (gray card), thresholds, and small re-trains per season/site.
                // - Monitor drift via validation images; update model or normalization if accuracy drops.
          </code></pre>
        </details>

        <p><strong>Deep Convolutional Neural Network RGB Classifiers</strong>: A neural network is trained on labeled images of produce at known ripeness levels. This technique can leverage transfer learning and imaging enhancements which allows models to better handle environmental variables, such as perspective or background changes than hand-crafted features.[<a href="#ref-8">8</a>]</p>
        <p><strong>Pros:</strong>
        <ul class="bullets">
            <li>Better handling of conplex scenes and situations.</li>
            <li>Can learn cues beyond color, such as texture or deeper color attributes such as speckling.</li>
        </ul>
        <p><strong>Cons:</strong>
        <ul class="bullets">
            <li>Requires strong training data and validation.</li>
            <li>Similar to the previous technique, does not go beyond surface-level detail.</li>
        </ul>
        <p></p>
        <details>
            <summary style="cursor:pointer;font-weight:600"><strong>Click to see algorithm</strong></summary>
            <pre><code>
                // Dataset (images + labels)
                data = load_images_with_labels(root_dir)              // e.g., classes: unripe, ripe, overripe
                split = stratified_train_val_test_split(data, 80/10/10)

                // Augmentation (improve robustness to real scenes)
                aug = Compose([
                  RandomResizedCrop(224), HorizontalFlip(p=0.5),
                  ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02),
                  GaussianBlur(p=0.2), Normalize(mean,std)
                ])

                // Model (transfer learning)
                backbone = load_pretrained("efficientnet_b0")         // or mobilenet_v3, resnet50, etc.
                freeze(backbone.features, until="last_stage")         // warmup: freeze most layers
                head = Sequential([GlobalPool(), Dropout(0.2), Dense(num_classes)])  // small linear head
                model = Model(backbone.features, head)

                // Training
                for epoch in 1..E:
                  for (x,y) in loader(train, aug):
                    y_hat = model(x)
                    loss = cross_entropy(y_hat, y)
                    loss.backward(); optimizer.step(); optimizer.zero_grad()
                  validate_on(val)                                    // track balanced accuracy, F1
                  unfreeze_progressively(backbone)                    // optional fine-tuning

                // Calibration (optional but recommended)
                cal_model = temperature_scale(model, val)             // or isotonic on logits

                // Export for edge
                edge_model = quantize_int8(cal_model, rep_data=small_calib_set)
                save(edge_model, "ripeness_rgb_classifier.tflite")    // or ONNX/CoreML/TensorRT

                // Inference (device)
                x = preprocess(capture_frame(), target=224)
                p = softmax(edge_model(x))
                pred = argmax(p)
                if max(p) &lt; 0.6: fallback = "ask for better lighting" // simple quality gate
                return {class: pred, confidence: max(p)}

                // Maintenance & drift handling
                // - Periodically collect hard cases, re-label small batches, fine-tune few epochs.
                // - Track per-site metrics; keep configs (exposure/WB) versioned for each camera.
            </code></pre>
        </details>

        <p><strong>Object detection and segmentation for defects with YOLO and Mask R-CNN</strong>: YOLO (You Only Look Once) works to identify bad spots, while Mask R-CNN (Mask Region-based Convolutional Neural Networks) works to color in damaged pixels when the image is processed. YOLO is good when trying to run detection quickly - eg reject if any produce has a bruise larger than X. This is often used during fruit processing when speed matters. Mask R-CNN is good when precision is required, for example if you want to reject if greater than N% of the surface of a piece of produce has mold or is discolored. Also good for reporting general quality.[<a href="#ref-9">9</a>]</p>
        <p><strong>Pros of YOLO:</strong>
        <ul class="bullets">
            <li>It's fast, simple to run directly on devices, and is robust after training.</li>
            <li>Handles variable backgrounds well.</li>
        </ul>
        <p><strong>Cons of YOLO:</strong>
        <ul class="bullets">
            <li>Not good for gathering precise data, like "what percentage of this fruit is moldy".</li>
            <li>Not as good as other models when it comes to picking up tiny defects.</li>
        </ul>
        <p><strong>Pros of Mask R-CNN:</strong>
        <ul class="bullets">
            <li>It's pixel-accurate and precise.</li>
            <li>Easy to configure - example, "reject if more than 15% mold".</li>
        </ul>
        <p><strong>Cons of Mask R-CNN:</strong>
        <ul class="bullets">
            <li>Slower and more heavy than other algorithms such as YOLO.</li>
            <li>Each defect region must be traced, which requires more difficult labeling.</li>
        </ul>

<!--        <div class="grid-2">-->
<!--            <figure>-->
<!--                <img loading="lazy" src="https://upload.wikimedia.org/wikipedia/commons/0/0c/Approximately_30_Gros_Michel_Bananas.jpg" alt="Banana examples for color/texture algorithms." />-->
<!--                <figcaption>Visible cues (color/texture) often suffice for bananas. Source: <a href="https://commons.wikimedia.org/wiki/File:Approximately_30_Gros_Michel_Bananas.jpg" target="_blank" rel="noopener">Wikimedia Commons</a></figcaption>-->
<!--            </figure>-->
<!--            <figure>-->
<!--                <img loading="lazy" src="https://upload.wikimedia.org/wikipedia/commons/2/2a/Spectral_sampling_RGB_multispectral_hyperspectral_imaging.svg" alt="Spectral sampling diagram." />-->
<!--                <figcaption>Spectral bands capture chemistry tied to ripeness. Source: <a href="https://commons.wikimedia.org/wiki/File:Spectral_sampling_RGB_multispectral_hyperspectral_imaging.svg" target="_blank" rel="noopener">Wikimedia Commons</a></figcaption>-->
<!--            </figure>-->
<!--        </div>-->

        <p></p>
        <details>
            <summary><strong>Conclusion: When to pick which algorithm?</strong></summary>
            <ul class="bullets">
                <li>Need where and how much? ‚Üí YOLO / Mask R-CNN.</li>
                <li>Need a robust ‚Äúthis item = X grade‚Äù in the wild? ‚Üí Deep RGB CNN</li>
                <li>Need fast, tiny, explainable in controlled light? ‚Üí Color/Texture + Basic ML</li>
            </ul>
        </details>

        <!-- Demo -->
        <section id="demo" class="content-section demo">
            <h3>Mini Demo: Simple RGB Heuristics + Tiny Classifier</h3>
            <p>Upload a banana photo (single fruit, front-lit). We estimate ripeness with a very simple approach: mean hue/brightness thresholds and a TF.js linear model you can train live with your own examples.</p>

            <div class="demo-ui">
                <input type="file" id="imgInput" accept="image/*" />
                <canvas id="preview" width="320" height="240" aria-label="Preview canvas"></canvas>
                <div class="result">
                    <div><strong>Heuristic Label:</strong> <span id="heurLabel">‚Äî</span></div>
                    <div><strong>Mean Hue:</strong> <span id="meanHue">‚Äî</span> | <strong>Brightness:</strong> <span id="meanVal">‚Äî</span></div>
                </div>
                <details class="trainbox">
                    <summary>Optional: train tiny TF.js classifier (live)</summary>
                    <p>Click to add a few examples, then train. The model sees <em>color histogram</em> features.</p>
                    <div class="buttons">
                        <button data-class="unripe" class="chip">Add UNRIPE</button>
                        <button data-class="ripe" class="chip">Add RIPE</button>
                        <button data-class="overripe" class="chip">Add OVERRIPE</button>
                        <button id="trainBtn" class="btn btn-primary">Train</button>
                    </div>
                    <div id="trainStatus" class="status">No samples yet.</div>
                    <div><strong>Model Prediction:</strong> <span id="modelPred">‚Äî</span></div>
                </details>
            </div>

            <pre class="code"><code>// Heuristic core (HSV thresholds):
// 1) Convert pixels to HSV, average hue (H) and value (V)
// 2) Simple rules (tune per setup):
//    if H in [25¬∞,75¬∞] & V>0.4 ‚Üí likely RIPE (yellow)
//    if H < 25¬∞ & V>0.35 ‚Üí OVERRIPE (brown)
//    if H > 75¬∞ & V>0.35 ‚Üí UNRIPE (green)
//    else ‚Üí uncertain (needs better lighting)
// Production tip: lock white balance & exposure, or include a gray card.</code></pre>
        </section>
<!--        <div class="note">-->
<!--            <strong>Example (quick, explainable RGB pipeline):</strong> see the interactive mini-demo below that thresholds hue/brightness and a tiny TF.js model to label ‚ÄúUnripe / Ripe / Overripe‚Äù.-->
<!--        </div>-->
    </section>

    <!-- Successful vs Less Successful -->
    <section id="success" class="content-section">
        <h3>Successful vs Less Successful Techniques</h3>
        <p>This is a deeper dive into one of the more common techniques: <strong>Classification Based on Color and Texture Using Basic ML</strong>.</p>
        <h3>Color/Texture Classification + Basic ML</h3>

        <h4>Where it succeeds</h4>
        <ul class="bullets" role="list">
            <li><strong>Produce with clear, monotonic color changes.</strong> Bananas and tomatoes are ideal examples.</li>
            <li><strong>Controlled light in clear, ideal situations.</strong> Use diffused, high-CRI LEDs and lock exposure and white balance.</li>
            <li><strong>Benchtop or light-tent with neutral background.</strong> Stable distance and a matte backdrop make texture cues consistent.</li>
            <li><strong>Low-data or quick prototyping.</strong> Tens to a few hundred labeled images per class works best.</li>
        </ul>

        <h4>Where it fails</h4>
        <ul class="bullets" role="list">
            <li><strong>Lighting drift and auto camera settings.</strong> Auto exposure or white balance shift pixel values and break thresholds and classifiers.</li>
            <li><strong>Specular glare, wax, and shadows.</strong> Pixels which misrepresent the fruit confuse color histograms and texture detection.</li>
            <li><strong>Green-to-green crops or internal changes.</strong> Crops which don't have much (or any) color change. RGB surface features can‚Äôt capture attributes such as firmness, early bruising.</li>
            <li><strong>Compression and poor quality capture of live imaging.</strong> JPEG artifacts and 8-bit limits smear texture and shift colors.</li>
        </ul>

        <!-- Color/Texture + Basic ML ‚Äî Success vs Failure -->
        <style>
  .ct-two { display:grid; grid-template-columns: repeat(auto-fit, minmax(280px,1fr)); gap:16px; }
  .ct-two figure { margin:0; border:1px solid #e5e7eb; border-radius:12px; overflow:hidden; background:#fff; }
  .ct-two img { width:100%; height:auto; display:block; }
  .ct-two figcaption { padding:10px 12px 12px; font:14px/1.45 system-ui, -apple-system, Segoe UI, Roboto, "Helvetica Neue", Arial, sans-serif; }
  .pill { display:inline-block; font-size:12px; padding:4px 8px; border-radius:999px; background:#eef2ff; color:#3730a3; margin-bottom:6px; }
  .attr { font-size:12px; color:#6b7280; margin-top:8px; }
  .attr a { color:#6b7280; text-decoration:underline; }
</style>

        <div class="ct-two">
            <!-- SUCCESS: clear, monotonic color + neutral background -->
            <figure>
                <img
                        src="https://commons.wikimedia.org/wiki/Special:FilePath/Banana-Single.jpg"
                        alt="Single ripe Cavendish banana on white background under controlled lighting"
                        loading="lazy">
                <figcaption>
                    <span class="pill">Success (ideal conditions)</span>
                    <div><strong>Why this works:</strong> Clear, monotonic color change on a neutral background with fixed lighting makes simple color-space stats (e.g., HSV or Lab means/histograms) and light texture cues highly separable for a small classifier.</div>
                    <div class="attr">Image: ‚ÄúBanana-Single‚Äù ‚Äî ¬© Evan-Amos ‚Äî License: CC BY-SA 3.0 ‚Äî <a href="https://commons.wikimedia.org/wiki/File:Banana-Single.jpg">Wikimedia file page</a></div>
                </figcaption>
            </figure>

            <!-- FAILURE: green-to-green/internal changes not visible in RGB -->
            <figure>
                <img
                        src="https://commons.wikimedia.org/wiki/Special:FilePath/Avocado%20Hass%20-%20single%20and%20halved.jpg"
                        alt="Whole and halved Hass avocado on gray: exterior reveals little about internal state"
                        loading="lazy">
                <figcaption>
                    <span class="pill">Failure (RGB surface limits)</span>
                    <div><strong>Why this fails:</strong> Green-to-green or low-contrast surfaces (e.g., Hass avocado) and internal changes (firmness, early bruising) are poorly expressed in RGB color/texture, so basic ML on surface pixels often misclassifies ripeness.</div>
                    <div class="attr">Image: ‚ÄúAvocado Hass ‚Äî single and halved‚Äù ‚Äî ¬© Ivar Leidus ‚Äî License: CC BY-SA 4.0 ‚Äî <a href="https://commons.wikimedia.org/wiki/File:Avocado_Hass_-_single_and_halved.jpg">Wikimedia file page</a></div>
                </figcaption>
            </figure>
        </div>
    </section>

    <!-- Challenges -->
    <section id="challenges" class="content-section">
        <h3>Challenges</h3>
        <p>There are a number of major challenges in this space.[<a href="#ref-10">2</a>,<a href="#ref-10">4</a>] These include:</p>
        <ul class="bullets">
            <li><strong>Human motion vs app latency:</strong> shaky phones and moving fruit results in motion blur. This can be solved with shorter exposure, stabilization, or burst frames.</li>
            <li><strong>Lighting drift and color consistency:</strong> Automated exposure hardware and software results in inconsistent color features. Locking camera settings can help.</li>
            <li><strong>Sensor alignment and calibration:</strong> For multispectral techniques, it's important to register bands and calibrate sensors regularly.</li>
            <li><strong>Domain shift:</strong> Different seasons, fruit variations, etc all require re-training.</li>
            <li><strong>Labels and ground truth:</strong> Models are only as good as the labels they learn from. If the ground truth is noisy or loosely defined (e.g., ‚Äúlooks ripe‚Äù), the model will learn that fuzziness.</li>
            <li><strong>Classification latency:</strong> Conveyor belts often need lightning-fast decisions but smaller models which would fit on mobile devices and return results faster are less accurate.</li>
        </ul>

    </section>

    <!-- Future & Conclusion -->
    <section id="future" class="content-section">
        <h3>Future Efforts & Conclusion</h3>
        <p><strong>Where research is heading:</strong> lighter snapshot HSI for field use, spectral-spatial transformers, self-supervised learning, and on-device models integrated with IoT/digital twins for farm-to-shelf traceability.</p>
        <ul class="bullets" role="list">
            <li><strong>Models that handle messy reality.</strong> Spectral‚Äìspatial deep nets, stronger transfer learning, and domain adaptation across variables.</li>
            <li><strong>Edge-first deployments.</strong> Quantized/lightweight detectors and instance segmentation on kiosks, packhouse PCs, and smart cameras with robust calibration/update playbooks.</li>
            <li><strong>From bins to forecasts.</strong> Move beyond stage labels to predicting <em>days-to-ripe</em> for logistics and dynamic pricing.</li>
            <li><strong>Robotics + vision in the canopy.</strong> Field robots/drones for pick/no-pick, thinning, and yield mapping, pushing sensing earlier in the season.</li>
        </ul>
        <p></p>
        <h3>Who‚Äôs pushing the frontier</h3>
        <ul class="bullets" role="list">
            <li><strong>UC Davis Postharvest Research &amp; Extension Center.</strong> Best practices for ripening/ethylene management, calibration, and QA that underpin robust deployments.</li>
            <li><strong>USDA Agricultural Research Service (ARS).</strong> Research into techniques for maturity and internal quality (e.g., SSC), including VIS‚ÄìNIR HSI for storage and shipping readiness.</li>
            <li><strong>Wageningen University &amp; Research.</strong> Non-destructive sensing via computer vision and ML.</li>
            <li><strong>Carnegie Mellon University (CMU) Robotics Institute.</strong> Agricultural perception/manipulation (Kantor Lab), canopy sensing, and on-farm sensor fusion that complements vision.</li>
        </ul>
        <p></p>
        <div class="grid-2 video-embed">
            <iframe title="Non-destructive fruit quality overview" loading="lazy" width="560" height="315"
                    src="https://www.youtube.com/embed/uekIq0SoIpY" allowfullscreen></iframe>
            <iframe title="HSI ripeness seminar" loading="lazy" width="560" height="315"
                    src="https://www.youtube.com/embed/G-CHevBpfBQ" allowfullscreen></iframe>
        </div>
    </section>


    <!-- Quiz -->
    <section id="quiz" class="content-section">
        <h3>Quick Check: 6-Question Quiz</h3>
        <form id="quizForm">
            <ol>
                <li>
                    <p>"Freshness" includes the evaluation and classification of: </p>
                    <label><input type="radio" name="q1" value="a" /> Auditory information</label>
                    <label><input type="radio" name="q1" value="b" /> Visible cues </label>
                    <label><input type="radio" name="q1" value="c" /> Weather reports</label>
                </li>
                <li>
                    <p>Which of the following is a commonly used sensor technique?</p>
                    <label><input type="radio" name="q2" value="a" /> Smell </label>
                    <label><input type="radio" name="q2" value="b" /> Images and video </label>
                    <label><input type="radio" name="q2" value="c" /> Vibes </label>
                </li>
                <li>
                    <p>One pro of the YOLO algorithm is:</p>
                    <label><input type="radio" name="q3" value="a" /> Gathering precise data, like "what percentage of this fruit is moldy?"</label>
                    <label><input type="radio" name="q3" value="b" /> It's fast, simple to run directly on devices, and is robust after training.</label>
                    <label><input type="radio" name="q3" value="c" /> It's accurate 100% of the time. </label>
                </li>
                <li>
                    <p>An example of where Color/Texture Classification and Basic ML succeeds is:</p>
                    <label><input type="radio" name="q4" value="a" /> Produce that has been dropped on the floor of the grocery store.</label>
                    <label><input type="radio" name="q4" value="b" /> Produce with clear, monotonic color changes.</label>
                    <label><input type="radio" name="q4" value="c" /> Produce that changes color from green to slightly more green.</label>
                </li>
                <li>
                    <p>Which item is a challenge in this problem space:</p>
                    <label><input type="radio" name="q5" value="a" /> Growing fruit that is both delicious and easy to peel.</label>
                    <label><input type="radio" name="q5" value="b" /> Human motion vs app latency: shaky phones and moving fruit results in motion blur. </label>
                    <label><input type="radio" name="q5" value="c" /> Trying to grow bananas in snowy climates.</label>
                </li>
                <li>
                    <p>Which of the following is a direction on future research:</p>
                    <label><input type="radio" name="q6" value="a" /> Spherical bananas that make classification easier.</label>
                    <label><input type="radio" name="q6" value="b" /> Models that handle messy reality. </label>
                    <label><input type="radio" name="q6" value="c" /> Using a rat's brain to run classification models.</label>
                </li>
            </ol>
            <button type="submit" class="btn btn-primary">Submit</button>
            <div id="quizScore" class="status"></div>
        </form>
    </section>

    <!-- Annotated bibliography -->
    <section id="references" class="content-section">
        <h2>References</h2>
        <ol>
            <li id="ref-1">
                <strong>[1] ‚ÄúFruit Ripening and Ethylene Management (with 7 color ripeness charts).‚Äù</strong>
                Mary Lu Arpaia, Beth Mitcham, Marita Cantwell, Carlos Crisosto, Adel Kader, Mike Reid, Jim Thompson (2008).
                UC Davis Postharvest Research &amp; Extension Center.
                <a href="https://postharvest.ucdavis.edu/publication/fruit-ripening-and-ethylene-management-7-color-ripeness-charts">Source</a><br/>
                <em>Synopsis:</em> Authoritative extension resource on ripening biology and practice: visual maturity indices (e.g., color charts),
                links to internal chemistry (sugars/acids/volatiles), temperature and ethylene effects, and practical measurement guidance.<br/>
                <em>Reliability:</em> <strong>High</strong> ‚Äî UC Davis Postharvest is a leading academic/industry authority with long-standing, citable guidance.
            </li>

            <li id="ref-2">
                <strong>[2] ‚ÄúInnovative Hyperspectral Imaging-Based Techniques for Quality Evaluation of Fruits and Vegetables: A Review.‚Äù</strong>
                Yuzhen Lu, Yuping Huang, Renfu Lu (2017). <em>Applied Sciences</em> 7(2):189 (MDPI).
                <a href="https://doi.org/10.3390/app7020189">DOI</a> |
                <a href="https://www.mdpi.com/2076-3417/7/2/189">Source</a><br/>
                <em>Synopsis:</em> Peer-reviewed review explaining why VIS‚ÄìNIR hyperspectral imaging is a fast, non-destructive approach that bridges imaging and spectroscopy to estimate internal attributes (e.g., SSC/firmness) and external defects; covers sensing modes and system design.<br/>
                <em>Reliability:</em> <strong>High</strong> ‚Äî Widely cited review authored by USDA-ARS/MSU researchers; rigorous and method-focused.
            </li>

            <li id="ref-3">
                <strong>[3] ‚ÄúApplication of Machine Vision System in Food Detection.‚Äù</strong>
                Zhifei Xiao, Jilai Wang, Lu Han, Shubiao Guo, Qinghao Cui (2022). <em>Frontiers in Nutrition</em> 9:888245.
                <a href="https://www.frontiersin.org/articles/10.3389/fnut.2022.888245/full">Source</a><br/>
                <em>Synopsis:</em> Peer-reviewed survey of computer vision for food quality/safety, detailing hardware, deep learning pipelines (CNNs),
                non-destructive inspection benefits, and practical challenges such as lighting, noise, motion, and environmental variability.<br/>
                <em>Reliability:</em> <strong>High</strong> ‚Äî Reputable open-access journal with transparent peer review; comprehensive citations and figures.
            </li>
            <li id="ref-4">
                <strong>[4] ‚ÄúArtificial Vision Systems for Fruit Inspection and Classification: Systematic Literature Review.‚Äù</strong>
                Ignacio Rojas Santelices, Sandra Cano, Fernando Moreira, √Ålvaro Pe√±a Fritz, <em>Sensors</em> 25(5):1524, 2025.
                <a href="https://doi.org/10.3390/s25051524">DOI</a> ¬∑
                <a href="https://www.mdpi.com/1424-8220/25/5/1524">Source</a><br/>
                <em>Synopsis:</em> A PRISMA-guided systematic review of fruit inspection/classification systems that reports
                (i) RGB cameras are the predominant sensor for fruit vision (‚âà84% of applications),
                (ii) smartphone-based acquisition is increasingly used at retail/points of consumption, and
                (iii) many studies capture under ambient light (e.g., sunlight) in orchards and retail, whereas processing lines prefer controlled LEDs‚Äîdirectly supporting common field/retail practices and their lighting dependencies.<br/>
                <em>Reliability:</em> <strong>High</strong> ‚Äî Peer-reviewed journal (MDPI <em>Sensors</em>), open access, systematic methodology with hardware/lighting breakdowns relevant to practical deployments.
            </li>
            <li id="ref-5">
                <strong>[5] ‚ÄúInnovative Hyperspectral Imaging-Based Techniques for Quality Evaluation of Fruits and Vegetables: A Review.‚Äù</strong>
                Yuzhen Lu, Yuping Huang, Renfu Lu. <em>Applied Sciences</em> 7(2):189, 2017. MDPI.
                <a href="https://doi.org/10.3390/app7020189">DOI</a> ¬∑ <a href="https://www.mdpi.com/2076-3417/7/2/189">Source</a><br/>
                <em>Synopsis:</em> Explains hyperspectral imaging (integration of imaging + spectroscopy), common configurations (including snapshot/area), and non-destructive estimation of internal attributes (e.g., soluble solids/firmness) for QC.<br/>
                <em>Reliability:</em> <strong>High</strong> ‚Äî Peer-reviewed review widely cited in food/produce sensing; authors include a USDA-ARS researcher.
            </li>

            <li id="ref-6">
                <strong>[6] ‚ÄúEvolving trends in fluorescence spectroscopy techniques for food analysis: a review.‚Äù</strong>
                (Multiple authors). <em>Trends in Food Science &amp; Technology</em>, 2024. Elsevier.
                <a href="https://www.sciencedirect.com/science/article/pii/S0889157524002461">Source</a><br/>
                <em>Synopsis:</em> Surveys fluorescence spectroscopy across meats/fish/oils and microorganisms, highlighting how emitted fluorescence tracks microbial growth and oxidation markers for rapid, non-destructive quality control.<br/>
                <em>Reliability:</em> <strong>High</strong> ‚Äî Recent peer-reviewed review in a top food-science journal.
            </li>

            <li id="ref-7">
                <strong>[7] ‚ÄúColour measurements by computer vision for food quality control ‚Äì A review.‚Äù</strong>
                C. Le√≥n et&nbsp;al. <em>Trends in Food Science &amp; Technology</em> 28(10): 2012. Elsevier.
                <a href="https://www.sciencedirect.com/science/article/pii/S0924224412001835">Source</a><br/>
                <em>Synopsis:</em> Reviews computer-vision color measurement, emphasizing perceptual color spaces (e.g., CIE Lab, HSV), histogram/moment features, texture descriptors, and the importance of controlled illumination and backgrounds.<br/>
                <em>Reliability:</em> <strong>High</strong> ‚Äî Established review with methodological guidance on features and lighting control.
            </li>

            <li id="ref-8">
                <strong>[8] ‚ÄúDeep learning in agriculture: A survey.‚Äù</strong>
                Andreas Kamilaris, Francesc X. Prenafeta-Bold√∫, 2018. arXiv (preprint of survey).
                <a href="https://arxiv.org/abs/1807.11809">Source</a><br/>
                <em>Synopsis:</em> Synthesizes dozens of agricultural CV studies; documents the effectiveness of CNNs, transfer learning, and data augmentation for robust performance versus classical hand-crafted features under real-world variability.<br/>
                <em>Reliability:</em> <strong>High</strong> ‚Äî Influential survey (also published version available via journal); transparent citations and comparisons.
            </li>

            <li id="ref-9">
                <strong>[9] ‚ÄúA review of external quality inspection for fruit grading using CNN.‚Äù</strong>
                (Multiple authors). <em>Engineering Applications of Artificial Intelligence</em>, 2024. Elsevier.
                <a href="https://www.sciencedirect.com/science/article/pii/S2589721724000369">Source</a><br/>
                <em>Synopsis:</em> Reviews CNN-based external quality control, covering object detection (e.g., YOLO) for fast inline decisions and instance/semantic segmentation (e.g., Mask R-CNN) for precise defect area quantification and grading rules.<br/>
                <em>Reliability:</em> <strong>High</strong> ‚Äî Recent peer-reviewed review focused on fruit grading; summarizes models, metrics, and deployment contexts.
            </li>
        </ol>
    </section>
</main>

<footer class="site-footer">
    <p>¬© <span id="year"></span> Produce Freshness Estimation Tutorial. Built for GitHub Pages.</p>
</footer>
</body>
</html>